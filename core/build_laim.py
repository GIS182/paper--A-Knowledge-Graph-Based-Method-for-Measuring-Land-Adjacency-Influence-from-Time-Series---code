import geopandas as gpd
import pandas as pd
import numpy as np
import logging
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple, Union, Optional
from scipy.spatial import cKDTree
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import Point
from itertools import product
from utils.timer import timeit
from utils.geodata_io import read_geodata
import multiprocessing as mp
from tqdm import tqdm
import psutil
import time
import re
import gc
from shapely.strtree import STRtree
from concurrent.futures import ProcessPoolExecutor, as_completed
from shapely import distance

logger = logging.getLogger(__name__)

def log_memory_usage():
    """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    mem = psutil.virtual_memory()
    logger.info(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {mem.used / 1024**3:.2f}GB/{mem.total / 1024**3:.2f}GB (å¯ç”¨: {mem.available / 1024**3:.2f}GB)")

def normalize_column_name(name):
    """ç»Ÿä¸€å­—æ®µåæ ¼å¼ï¼ˆå°å†™+å»ç‰¹æ®Šå­—ç¬¦ï¼‰"""
    return re.sub(r'[^a-z0-9]', '', name.lower().strip())

def find_column_by_normalized(df, target):
    """é€šè¿‡æ ‡å‡†åŒ–åç§°æŸ¥æ‰¾å­—æ®µ"""
    target_norm = normalize_column_name(target)
    for col in df.columns:
        if normalize_column_name(col) == target_norm:
            return col
    return None

@timeit("åŠ è½½æƒé‡é…ç½®")
def load_weights(json_path: Union[str, Path] = "config/weight_config.json") -> Dict[str, float]:
    """
    åŠ è½½LAIMæƒé‡é…ç½®
    """
    json_path = Path(json_path)
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            weights = json.load(f)['laim_weights']
        logger.info(f"âœ… åŠ è½½LAIMæƒé‡: {weights}")
        return weights
    except (FileNotFoundError, KeyError, json.JSONDecodeError) as e:
        logger.critical(f"âŒ LAIMæƒé‡é…ç½®åŠ è½½å¤±è´¥: {json_path} | é”™è¯¯: {str(e)}")
        # è¿”å›é»˜è®¤æƒé‡é˜²æ­¢ä¸­æ–­
        return {'avg_shortest_distance': 0.4, 'centroid_distance': 0.3, 'boundary_distance': 0.3}

def validate_input(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """åŠ¨æ€ä¿®å¤ç¼ºå¤±å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰å¹¶ç¡®ä¿åæ ‡ç³»ä¸€è‡´"""
    # 1. æŸ¥æ‰¾land_codeå­—æ®µï¼ˆå…¼å®¹ä¸åŒå‘½åï¼‰
    land_code_col = find_column_by_normalized(gdf, "land_code")
    if not land_code_col:
        raise KeyError("âŒ æ•°æ®ä¸­æœªæ‰¾åˆ°land_codeå­—æ®µ")

    # é‡å‘½åç»Ÿä¸€å­—æ®µå
    if land_code_col != "land_code":
        gdf = gdf.rename(columns={land_code_col: "land_code"})

    # 2. å¼ºåˆ¶ç»Ÿä¸€æ•°æ®ç±»å‹
    gdf['land_code'] = gdf['land_code'].astype(int)

    # 3. åæ ‡ç³»ç»Ÿä¸€ä¸ºEPSG:4547
    if gdf.crs != 'EPSG:4547':
        logger.warning(f"âš ï¸ é‡æŠ•å½±è‡³EPSG:4547 (åŸå§‹CRS: {gdf.crs})")
        gdf = gdf.to_crs('EPSG:4547')

    # 4. è‡ªåŠ¨è®¡ç®—ç¼ºå¤±çš„é‡å¿ƒåæ ‡
    if 'centroid_x' not in gdf.columns or 'centroid_y' not in gdf.columns:
        logger.warning("âš ï¸ è‡ªåŠ¨ä¿®å¤ï¼šç¼ºå°‘é‡å¿ƒåæ ‡å­—æ®µï¼Œé‡æ–°è®¡ç®—...")
        gdf['centroid_x'] = gdf.geometry.centroid.x
        gdf['centroid_y'] = gdf.geometry.centroid.y

    return gpd.GeoDataFrame(gdf, geometry='geometry')

# è®¡ç®—ä¸¤ç»„é‡å¿ƒä¹‹é—´çš„å¹³å‡æ¬§æ°è·ç¦»
def calculate_centroid_distance_matrix(centroids: Dict[int, np.ndarray]) -> Dict[Tuple[int, int], float]:
    """
    é‡å¿ƒè·ç¦»è®¡ç®—ï¼ˆé¿å…O(nÂ²)å†…å­˜çˆ†ç‚¸ï¼‰
    ä½¿ç”¨KDæ ‘+æŠ½æ ·ç»Ÿè®¡æ›¿ä»£å…¨çŸ©é˜µè®¡ç®—
    """
    if not centroids:
        logger.error("âŒ ç©ºé‡å¿ƒæ•°æ®ï¼Œæ— æ³•è®¡ç®—è·ç¦»")
        return {}

    logger.info(f"ğŸ“ è®¡ç®—é‡å¿ƒè·ç¦»çŸ©é˜µ | åœ°ç±»æ•°: {len(centroids)}")
    centroid_distances = {}
    SAMPLE_SIZE = 500  # æ¯ç±»æœ€å¤§é‡‡æ ·ç‚¹æ•°é‡ï¼ˆæ§åˆ¶å†…å­˜ï¼‰

    all_dists = []
    for code_i, points_i in centroids.items():
        if len(points_i) == 0:
            continue

        # å¯¹å¤§å‹åœ°ç±»æŠ½æ ·ï¼ˆé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
        if len(points_i) > SAMPLE_SIZE:
            idx = np.random.choice(len(points_i), SAMPLE_SIZE, replace=False)
            points_i = points_i[idx]

        for code_j, points_j in centroids.items():
            if code_i == code_j or len(points_j) == 0:
                continue

            # å¯¹å¤§å‹åœ°ç±»æŠ½æ ·
            if len(points_j) > SAMPLE_SIZE:
                idx = np.random.choice(len(points_j), SAMPLE_SIZE, replace=False)
                points_j = points_j[idx]

            # æ„å»ºKDæ ‘æŸ¥è¯¢æœ€è¿‘é‚»
            tree = cKDTree(points_j.astype(np.float32))
            dists, _ = tree.query(points_i, k=1, workers=1)
            avg_dist = np.mean(dists)
            centroid_distances[(code_i, code_j)] = avg_dist
            all_dists.append(avg_dist)

    global_avg = np.mean(all_dists) if all_dists else 1000.0
    logger.info(f"âœ… ç”Ÿæˆ {len(centroid_distances)} ä¸ªè·ç¦»å…³ç³» | å…¨å±€å¹³å‡è·ç¦»: {global_avg:.2f}m")
    return centroid_distances

# é‡æ„è¾¹ç•Œè·ç¦»è®¡ç®—å‡½æ•°
def extract_polygons(geoms: gpd.GeoSeries, sample_size: int = 100) -> List[Polygon]:
    """
    æ­£ç¡®æå–å¤šè¾¹å½¢è¾¹ç•Œï¼ˆè§£å†³ç©ºè¾¹ç•Œé—®é¢˜ï¼‰
    """
    valid_polys = []
    for geom in geoms:
        # è·³è¿‡æ— æ•ˆå‡ ä½•
        if geom.is_empty or not geom.is_valid:
            continue

        # å¤„ç†MultiPolygonç±»å‹
        if geom.geom_type == 'MultiPolygon':
            for poly in geom.geoms:
                if isinstance(poly, Polygon) and not poly.is_empty:
                    valid_polys.append(poly)
        # å¤„ç†Polygonç±»å‹
        elif geom.geom_type == 'Polygon':
            valid_polys.append(geom)
        # è·³è¿‡å…¶ä»–ç±»å‹
        else:
            logger.warning(f"âš ï¸ è·³è¿‡éå¤šè¾¹å½¢å‡ ä½•ç±»å‹: {geom.geom_type}")
    return valid_polys[:min(len(valid_polys), sample_size)]

def _calc_pair_distance(args):
    """
    è®¡ç®—ä¸¤ä¸ªåœ°ç±»å¤šè¾¹å½¢ä¹‹é—´çš„æœ€çŸ­è·ç¦»ï¼ˆæ›¿ä»£å…±äº«è¾¹ç•Œï¼‰
    """
    code_i, code_j, tree_i, geoms_j = args
    try:
        if code_i == code_j:
            return (code_i, code_j), 0.0

        total_dist = 0.0
        count = 0
        for geom_j in geoms_j:
            # ä¸¥æ ¼å‡ ä½•éªŒè¯
            if not isinstance(geom_j, (Polygon, MultiPolygon)):
                continue
            if geom_j.is_empty or not geom_j.is_valid:
                continue

            # æŸ¥è¯¢æœ€è¿‘å‡ ä½•
            nearest_geom = tree_i.nearest(geom_j)

            # éªŒè¯æœ€è¿‘å‡ ä½•æœ‰æ•ˆæ€§
            if not isinstance(nearest_geom, (Polygon, MultiPolygon)) or nearest_geom.is_empty:
                continue

            dist = distance(geom_j, nearest_geom)
            total_dist += dist
            count += 1

        return (code_i, code_j), total_dist / count if count > 0 else 0.0
    except Exception as e:
        logger.error(f"è¾¹ç•Œè·ç¦»è®¡ç®—å¤±è´¥ {code_i}-{code_j}: {str(e)}")
        return (code_i, code_j), 0.0

# è®¡ç®—è¾¹ç•Œé—´æœ€è¿‘è·ç¦»ï¼ˆæ‰€æœ‰è¾¹ç•Œå¯¹ï¼‰
def calculate_boundary_distance_matrix(class_geoms: Dict[int, gpd.GeoSeries]) -> Dict[Tuple[int, int], float]:
    """
    ä½¿ç”¨ç©ºé—´ç´¢å¼•ä¼˜åŒ–è¾¹ç•Œè·ç¦»è®¡ç®—
    ç­–ç•¥ï¼šä½¿ç”¨STRtreeç´¢å¼•+æŠ½æ ·ç­–ç•¥+å¤šè¿›ç¨‹
    """
    if not class_geoms:
        logger.error("âŒ ç©ºå‡ ä½•æ•°æ®ï¼Œæ— æ³•è®¡ç®—è¾¹ç•Œè·ç¦»")
        return {}

    logger.info("ğŸ“ è¾¹ç•Œè·ç¦»çŸ©é˜µè®¡ç®—...")
    SAMPLE_SIZE = 100  # æ¯ç±»æœ€å¤§å‡ ä½•æ ·æœ¬é‡
    NUM_WORKERS = min(4, os.cpu_count())  # æ§åˆ¶è¿›ç¨‹æ•°

    # æ„å»ºç©ºé—´ç´¢å¼•å­—å…¸ {code: (STRtree, å‡ ä½•åˆ—è¡¨)}
    spatial_data = {}
    for code, geoms in class_geoms.items():
        if not geoms.empty:
            polygons = extract_polygons(geoms, SAMPLE_SIZE)
            if polygons:
                spatial_data[code] = (STRtree(polygons), polygons)

    # å‡†å¤‡å¤šè¿›ç¨‹ä»»åŠ¡
    tasks = []
    codes = list(spatial_data.keys())
    for code_i in codes:
        tree_i, geoms_i = spatial_data[code_i]
        for code_j in codes:
            _, geoms_j = spatial_data.get(code_j, (None, []))
            tasks.append((code_i, code_j, tree_i, geoms_j))

    # å¤šè¿›ç¨‹è®¡ç®—
    boundary_distances = {}
    logger.info(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹è®¡ç®— | ä»»åŠ¡æ•°: {len(tasks)} | è¿›ç¨‹æ•°: {NUM_WORKERS}")

    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        futures = {executor.submit(_calc_pair_distance, task): task for task in tasks}
        for future in tqdm(as_completed(futures), total=len(futures), desc="è¾¹ç•Œè·ç¦»è®¡ç®—"):
            (code_i, code_j), dist = future.result()
            boundary_distances[(code_i, code_j)] = dist

    logger.info(f"âœ… è¾¹ç•Œè·ç¦»çŸ©é˜µå®Œæˆ | ç»„åˆæ•°: {len(boundary_distances)}")
    return boundary_distances

# è®¡ç®—å¹³å‡æœ€çŸ­é‚»æ¥è·ç¦»ï¼ˆé‡å¿ƒä¹‹é—´çš„æœ€è¿‘è·ç¦»å¯¹ï¼‰
def calculate_nearest_neighbor_matrix(class_centroids: Dict[int, np.ndarray], threshold=300) -> Dict[
    Tuple[int, int], float]:
    """
    æ‰¹é‡è®¡ç®—æ‰€æœ‰åœ°ç±»å¯¹ä¹‹é—´çš„å¹³å‡æœ€è¿‘é‚»è·ç¦»
    é¿å…é‡å¤æ„å»ºKDTree
    nn_distancesæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨æ‰€æœ‰åœ°ç±»å¯¹ä¹‹é—´çš„æœ€è¿‘é‚»è·ç¦»è®¡ç®—ç»“æœï¼Œé”®ä¸º (code_i, code_j)ï¼Œå€¼ä¸ºè·ç¦»å€¼ã€‚
    å®ƒæ˜¯ä¸€ä¸ªä¸­é—´è®¡ç®—ç»“æœå®¹å™¨ã€‚nn_distancesæ˜¯å­—å…¸ï¼ˆæ•°æ®é›†åˆï¼‰ï¼Œè€Œ avg_shortest_distanceæ˜¯æµ®ç‚¹æ•°ï¼ˆæƒé‡å€¼ï¼‰
    """
    if not class_centroids:
        logger.error("âŒ ç©ºé‡å¿ƒæ•°æ®ï¼Œæ— æ³•è®¡ç®—æœ€è¿‘é‚»è·ç¦»")
        return {}

    logger.info("ğŸ” æœ€è¿‘é‚»è·ç¦»çŸ©é˜µè®¡ç®—...")
    nn_distances = {}

    # 1. æ„å»ºæ‰€æœ‰åœ°ç±»çš„KDæ ‘ï¼ˆé¿å…é‡å¤æ„å»ºï¼‰
    trees = {}
    for code, points in class_centroids.items():
        if points.size > 0:
            trees[code] = cKDTree(points.astype(np.float32))

    # 2. è®¡ç®—å…¨å±€å¹³å‡è·ç¦»ï¼ˆç”¨äºå¡«å……æ— æ•ˆå€¼ï¼‰
    global_avg = 0.0
    valid_pairs = 0

    # 3. è®¡ç®—åœ°ç±»å¯¹è·ç¦»
    codes = list(trees.keys())
    for code_i in codes:
        tree_i = trees[code_i]
        for code_j in codes:
            if code_i == code_j:
                nn_distances[(code_i, code_j)] = 0.0
                continue

            tree_j = trees.get(code_j)
            if tree_j is None:
                continue

            # æŸ¥è¯¢code_iåˆ°code_jçš„æœ€è¿‘é‚»
            dists, _ = tree_i.query(tree_j.data, k=1, distance_upper_bound=threshold)
            valid_dists = dists[dists < threshold]

            if valid_dists.size > 0:
                avg_dist = np.mean(valid_dists)
                nn_distances[(code_i, code_j)] = avg_dist
                global_avg += avg_dist
                valid_pairs += 1

    # 4. å¤„ç†æ— æ•ˆå€¼
    global_avg = global_avg / valid_pairs if valid_pairs > 0 else 150.0
    for pair in [(i, j) for i in codes for j in codes]:
        if pair not in nn_distances:
            nn_distances[pair] = global_avg

    return nn_distances

def save_laim_matrix(df: pd.DataFrame, value_col: str, output_path: Union[str, Path]) -> None:
    """
    ä¿å­˜ä¸ºé‚»æ¥çŸ©é˜µæ ¼å¼ï¼ˆå¯¹è§’çº¿ç½®0ï¼‰
    ä½¿ç”¨pivot_tableæ›¿ä»£å¾ªç¯
    """
    output_path = Path(output_path)
    logger.info("ğŸ“ æ­£åœ¨è½¬æ¢ä¸ºé‚»æ¥å½±å“çŸ©é˜µæ ¼å¼...")

    # å…³é”®å­—æ®µå­˜åœ¨æ€§æ£€æŸ¥
    required_cols = ['class_a', 'class_b', value_col]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logger.critical(f"âŒ ç¼ºå¤±å…³é”®å­—æ®µ: {missing_cols}ï¼Œæ— æ³•ç”ŸæˆçŸ©é˜µ")
        return

    # åˆ›å»ºé€è§†è¡¨
    pivot_df = df.pivot_table(index='class_a', columns='class_b', values=value_col, fill_value=0)

    # ç¡®ä¿æ‰€æœ‰åœ°ç±»éƒ½åœ¨è¡Œåˆ—ä¸­
    all_codes = sorted(set(df['class_a']).union(df['class_b']))
    pivot_df = pivot_df.reindex(index=all_codes, columns=all_codes, fill_value=0)

    # å¯¹è§’çº¿ç½®é›¶
    for code in all_codes:
        if code in pivot_df.index and code in pivot_df.columns:
            pivot_df.loc[code, code] = 0

    pivot_df.to_csv(output_path)
    logger.info(f"âœ… å·²ä¿å­˜åœ°ç±»é‚»æ¥å½±å“çŸ©é˜µè‡³ {output_path.name}")

@timeit("æ„å»ºåœ°ç±»é‚»æ¥å½±å“å›¾è°± LAIM")
def build_laim(input_dirs: List[Union[str, Path]], output_dir: Union[str, Path]) -> None:
    """
    æ„å»º LAIM å›¾è°±ï¼ˆæ”¯æŒå¤šæœŸæ•°æ®ï¼‰

        1. å¢é‡åŠ è½½æ•°æ®ï¼Œå‡å°‘å†…å­˜å³°å€¼
        2. é¢„èšåˆåœ°ç±»å‡ ä½•å’Œé‡å¿ƒ
        3. å¹¶è¡Œè®¡ç®—ç‹¬ç«‹æŒ‡æ ‡
    """
    log_memory_usage()
    weights = load_weights()
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ­¥éª¤1: å¢é‡åŠ è½½æ•°æ®å¹¶é¢„èšåˆ
    class_centroids = {}  # {land_code: numpyæ•°ç»„(N,2)}
    class_geoms = {}  # {land_code: GeoSeries}

    logger.info("ğŸ“¥ å¢é‡åŠ è½½å¤šæœŸåˆ†ç±»å›¾å±‚...")
    input_dirs = [Path(d) for d in input_dirs]

    # è·å–æ‰€æœ‰æœ‰æ•ˆåœŸåœ°ç±»å‹ï¼ˆ1-8ï¼Œæ’é™¤6ï¼‰
    VALID_CODES = {1, 2, 3, 4, 5, 7, 8}

    def check_memory(min_avail_gb=1.0):
        """å†…å­˜è­¦æˆ’çº¿æ£€æŸ¥ï¼ˆéé˜»å¡ï¼‰"""
        mem = psutil.virtual_memory()
        avail_gb = mem.available / 1024**3
        if avail_gb < min_avail_gb:
            logger.warning(f"ğŸ›‘ å¯ç”¨å†…å­˜ä½äº{min_avail_gb}GB ({avail_gb:.2f}GB)ï¼Œè§¦å‘GCå›æ”¶")
            gc.collect()
            new_mem = psutil.virtual_memory()
            logger.info(f"â™»ï¸ GCåå¯ç”¨å†…å­˜: {new_mem.available/1024**3:.2f}GB")
        return mem.available

    for dir_path in tqdm(input_dirs, desc="åŠ è½½ç›®å½•"):
        # è·å–æ‰€æœ‰åœ°ç±»å›¾å±‚æ–‡ä»¶ï¼ˆæ”¯æŒ.gpkgå’Œ.shpï¼‰
        files = list(dir_path.glob("*.gpkg")) + list(dir_path.glob("*.shp"))
        if not files:
            logger.warning(f"âš ï¸ ç›®å½•ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶: {dir_path}")
            continue

        for file_path in files:
            try:
                # ç§»é™¤é˜»å¡æ€§sleepï¼Œæ”¹ä¸ºè½»é‡çº§å†…å­˜æ£€æŸ¥
                avail_mem = check_memory(1.0)
                if avail_mem < 1.5 * 1024 ** 3:
                    logger.warning(f"ğŸ›‘ å¯ç”¨å†…å­˜ä½äº1.5GB ({avail_mem / 1024 ** 3:.2f}GB)ï¼Œè·³è¿‡å½“å‰æ–‡ä»¶")
                    continue  # è·³è¿‡å½“å‰æ–‡ä»¶ä½†ä¸é˜»å¡

                layer_name = file_path.stem
                logger.info(f"ğŸ” åŠ è½½æ–‡ä»¶: {file_path.name}|{layer_name}")
                gdf = read_geodata(str(file_path), layer=layer_name)
                gdf = validate_input(gdf)

                gdf['centroid_x'] = gdf['centroid_x'].astype(np.float32)
                gdf['centroid_y'] = gdf['centroid_y'].astype(np.float32)

                # æŒ‰åœ°ç±»åˆ†ç»„èšåˆ
                for land_code, group in gdf.groupby('land_code'):
                    if land_code not in VALID_CODES:
                        continue

                    # è·å–å½“å‰åœ°ç±»çš„é‡å¿ƒç‚¹é›†
                    centroids = group[['centroid_x', 'centroid_y']].values
                    if land_code not in class_centroids:
                        class_centroids[land_code] = centroids
                    else:
                        class_centroids[land_code] = np.vstack([class_centroids[land_code], centroids])

                    # ç”¨concatæ›¿ä»£appendï¼ˆè§£å†³GeoSeriesæ— appendæ–¹æ³•ï¼‰
                    if land_code not in class_geoms:
                        class_geoms[land_code] = group.geometry
                    else:
                        class_geoms[land_code] = pd.concat([class_geoms[land_code], group.geometry])

                # åŠæ—¶é‡Šæ”¾å†…å­˜
                del gdf
                gc.collect()

            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path} | é”™è¯¯: {str(e)}")
                continue

    # è¿‡æ»¤ç©ºæ•°æ®
    valid_codes = [code for code in class_centroids if class_centroids[code].size > 0]
    class_centroids = {code: class_centroids[code] for code in valid_codes}
    class_geoms = {code: class_geoms[code] for code in valid_codes}
    logger.info(f"ğŸ“Š æœ‰æ•ˆåœ°ç±»æ•°: {len(valid_codes)} | åœ°ç±»åˆ—è¡¨: {valid_codes}")

    # å…³é”®æ£€æŸ¥ï¼šæœ‰æ•ˆåœ°ç±»æ•°ä¸º0æ—¶ç»ˆæ­¢
    if not valid_codes:
        logger.critical("âŒ è‡´å‘½é”™è¯¯ï¼šæœªå‘ç°æœ‰æ•ˆåœ°ç±»ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®")
        raise ValueError("è¾“å…¥æ•°æ®ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„land_codeå­—æ®µæˆ–æ‰€æœ‰åœ°ç±»æ•°æ®ä¸ºç©º")

    # æ­¥éª¤2: è®¡ç®—ä¸‰ç±»ç©ºé—´æŒ‡æ ‡
    logger.info("â³ è®¡ç®—åœ°ç±»é—´ç©ºé—´å…³ç³»æŒ‡æ ‡...")
    # 1. å…ˆè®¡ç®—è¾¹ç•Œè·ç¦»ï¼ˆéœ€è¦å‡ ä½•æ•°æ®ï¼‰
    boundary_dists = calculate_boundary_distance_matrix(class_geoms)

    # 2. å†è®¡ç®—é‡å¿ƒè·ç¦»
    centroid_dists = calculate_centroid_distance_matrix(class_centroids)

    # 3. æœ€åè®¡ç®—æœ€è¿‘é‚»è·ç¦»ï¼ˆé‡Šæ”¾å‡ ä½•æ•°æ®ï¼‰
    nn_dists = calculate_nearest_neighbor_matrix(class_centroids)
    del class_centroids, class_geoms # åŠæ—¶é‡Šæ”¾å†…å­˜
    gc.collect()

    # ç©ºç»“æœæ£€æŸ¥
    if not centroid_dists or not boundary_dists or not nn_dists:
        logger.critical("âŒ ç©ºé—´æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§")
        raise RuntimeError("ç©ºé—´å…³ç³»æŒ‡æ ‡è®¡ç®—ç»“æœä¸ºç©º")

    # æ­¥éª¤3: åˆå¹¶ç»“æœ
    results = []
    all_pairs = set(centroid_dists.keys()) | set(boundary_dists.keys()) | set(nn_dists.keys())
    for (class_a, class_b) in all_pairs:
        results.append({
            'class_a': class_a,
            'class_b': class_b,
            'centroid_distance': centroid_dists.get((class_a, class_b), 0),
            'boundary_distance': boundary_dists.get((class_a, class_b), 0),
            'avg_shortest_distance': nn_dists.get((class_a, class_b), 0)
        })

    df = pd.DataFrame(results)
    logger.info(f"ğŸ“¦ åˆå¹¶ç»“æœå®Œæˆ | å…³ç³»å¯¹æ•°: {len(df)}")

    # ç©ºæ•°æ®æ¡†æ£€æŸ¥
    if df.empty:
        logger.critical("âŒ åˆå¹¶ç»“æœä¸ºç©ºï¼Œæ— æ³•ç»§ç»­å¤„ç†")
        raise ValueError("åœ°ç±»å…³ç³»å¯¹æ•°æ®æ¡†ä¸ºç©º")

    # æ­¥éª¤4: å½’ä¸€åŒ–å¤„ç†
    logger.info("ğŸ“Š æ‰§è¡Œå½’ä¸€åŒ–å¤„ç†...")

    # å­—æ®µå­˜åœ¨æ€§æ£€æŸ¥å’Œè‡ªåŠ¨è¡¥å…¨
    required_cols = ['centroid_distance', 'boundary_distance', 'avg_shortest_distance']
    for col in required_cols:
        if col not in df.columns:
            logger.warning(f"âš ï¸ ç¼ºå¤±å­—æ®µè‡ªåŠ¨è¡¥å…¨: {col}")
            df[col] = 0

    # è®°å½•åŸå§‹æŒ‡æ ‡èŒƒå›´
    original_min_max = {}
    for col in required_cols:
        min_val = df[col].min()
        max_val = df[col].max()

        # é˜²æ­¢æç«¯å€¼å½±å“å½’ä¸€åŒ–
        if max_val - min_val < 1e-6:
            min_val = 0
            max_val = max_val if max_val > 0 else 1.0

        original_min_max[col] = (min_val, max_val)
        df[f"{col}_norm"] = (df[col] - min_val) / (max_val - min_val + 1e-8)  # é¿å…é™¤é›¶

    # æ­¥éª¤5: åŠ æƒåˆæˆLAIæŒ‡æ•°
    logger.info("âš–ï¸ æ‰§è¡Œæƒé‡åŠ æƒå åŠ ç”ŸæˆLAI_normå€¼...")
    df["LAI_norm"] = (
            weights["centroid_distance"] * df["centroid_distance_norm"] +
            weights["boundary_distance"] * df["boundary_distance_norm"] +
            weights["avg_shortest_distance"] * df["avg_shortest_distance_norm"]
    )

    # æ­¥éª¤6: åå½’ä¸€åŒ–
    logger.info("âš™ï¸ æ‰§è¡ŒLAIå€¼åå½’ä¸€åŒ–...")
    min_possible = sum(
        weights[col] * original_min_max[col][0]
        for col in required_cols
    )
    max_possible = sum(
        weights[col] * original_min_max[col][1]
        for col in required_cols
    )

    df["LAI"] = (df["LAI_norm"] * (max_possible - min_possible) + min_possible)

    # æ­¥éª¤7: ä¿å­˜ç»“æœ
    pairs_path = output_dir / "laim_pairs.csv"
    matrix_path = output_dir / "laim_matrix.csv"
    lcs_matrix_path = output_dir / "lcsm_matrix.csv"

    df.to_csv(pairs_path, index=False)
    logger.info(f"âœ… å·²ä¿å­˜LAIMå…³ç³»å¯¹è¡¨: {pairs_path.name}")

    save_laim_matrix(df, "LAI", matrix_path)
    log_memory_usage()