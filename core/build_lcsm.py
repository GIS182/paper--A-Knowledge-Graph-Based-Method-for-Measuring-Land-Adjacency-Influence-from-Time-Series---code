import geopandas as gpd
import pandas as pd
from rasterio.transform import from_origin
from rasterio.features import rasterize
import numpy as np
import logging
import json
import time
from pathlib import Path
from shapely import LineString
from shapely.ops import unary_union
from typing import List, Dict, Union
from concurrent.futures import ProcessPoolExecutor
from shapely.strtree import STRtree
from scipy.ndimage import generic_filter
from utils.timer import timeit
from utils.geodata_io import read_geodata
import psutil
import re
import gc
import sys
from multiprocessing import Process, Queue

logger = logging.getLogger(__name__)

def log_memory_usage():
    """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    mem = psutil.virtual_memory()
    logger.info(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {mem.used / 1024**3:.2f}GB/{mem.total / 1024**3:.2f}GB (å¯ç”¨: {mem.available / 1024**3:.2f}GB)")

def normalize_column_name(name):
    """ç»Ÿä¸€å­—æ®µåæ ¼å¼ï¼ˆå°å†™+å»ç‰¹æ®Šå­—ç¬¦ï¼‰"""
    return re.sub(r'[^a-z0-9]', '', name.lower().strip())

def find_column_by_normalized(df, target):
    """é€šè¿‡æ ‡å‡†åŒ–åç§°æŸ¥æ‰¾å­—æ®µ"""
    target_norm = normalize_column_name(target)
    for col in df.columns:
        if normalize_column_name(col) == target_norm:
            return col
    return None

@timeit("åŠ è½½æƒé‡é…ç½®")
def load_weights(json_path: Union[str, Path] = "config/weight_config.json") -> Dict[str, float]:
    """
    åŠ è½½LCSMæƒé‡é…ç½®
    """
    json_path = Path(json_path)
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # è®¾ç½®é»˜è®¤å€¼ï¼ˆé˜²æ­¢é…ç½®ç¼ºå¤±ï¼‰
        config.setdefault('lcsm_distance_range', [10, 300])
        config.setdefault('distance_decay', {'a': 1000, 'b': 0.5})
        config.setdefault('impact_decay_factor', 0.01)
        config.setdefault('lcsm_weights', {
            "transition_frequency": 0.5,
            "contact_density": 0.3,
            "mixture_index": 0.2
        })

        logger.info(f"âœ… åŠ è½½LCSMé…ç½®: { {k: v for k, v in config.items() if k != 'laim_weights'} }")
        return config
    except Exception as e:
        logger.critical(f"âŒ LCSMé…ç½®åŠ è½½å¤±è´¥: {json_path} | é”™è¯¯: {str(e)}")
        # è¿”å›å®‰å…¨çš„é»˜è®¤é…ç½®
        return {
            'lcsm_weights': {"transition_frequency": 0.5, "contact_density": 0.3, "mixture_index": 0.2},
            'lcsm_distance_range': [10, 300],
            'distance_decay': {'a': 1000, 'b': 0.5},
            'impact_decay_factor': 0.01
        }

def validate_dataframe(df: pd.DataFrame, required_cols: List[str]) -> pd.DataFrame:
    """éªŒè¯DataFrameæ˜¯å¦åŒ…å«æ‰€éœ€å­—æ®µï¼Œç¼ºå¤±åˆ™å¡«å……é»˜è®¤å€¼0"""
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logger.warning(f"âš ï¸ æ•°æ®æ¡†ç¼ºå¤±å­—æ®µ: {missing_cols}ï¼Œè‡ªåŠ¨å¡«å……é»˜è®¤å€¼0")
        for col in missing_cols:
            df[col] = 0
    return df

# è®¡ç®—åœ°ç±»è½¬åŒ–é¢‘ç‡ T(i,j)
def compute_transition_frequency(gdf_list: List[gpd.GeoDataFrame], resolution: int = 30) -> pd.DataFrame:
    """
    æ …æ ¼åŒ–æ–¹æ³•è®¡ç®—åœ°ç±»è½¬åŒ–é¢‘ç‡ T(i,j)
    ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
    """
    # éªŒè¯è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
    if len(gdf_list) < 2:
        logger.error("âŒ è‡³å°‘éœ€è¦ä¸¤ä¸ªæ—¶é—´æ®µçš„æ•°æ®æ‰èƒ½è®¡ç®—åœ°ç±»è½¬åŒ–é¢‘ç‡")
        return pd.DataFrame(columns=['class_a', 'class_b', 'T'])

    transition_counts = {}
    total_transitions = 0

    for i in range(len(gdf_list) - 1):
        gdf1, gdf2 = gdf_list[i], gdf_list[i + 1]
        bounds = gpd.GeoSeries([gdf1.unary_union, gdf2.unary_union]).unary_union.bounds
        transform = from_origin(bounds[0], bounds[3], resolution, resolution)
        width = int((bounds[2] - bounds[0]) / resolution) + 1
        height = int((bounds[3] - bounds[1]) / resolution) + 1

        # æ …æ ¼åŒ–ï¼ˆä½¿ç”¨æ•´å‹ç¼–ç ï¼‰
        raster1 = rasterize(
            [(geom, int(code)) for geom, code in zip(gdf1.geometry, gdf1.land_code)],
            out_shape=(height, width),
            transform=transform,
            fill=0,
            dtype=np.int32  # æ”¹ä¸º32ä½é¿å…æº¢å‡º
        )
        raster2 = rasterize(
            [(geom, int(code)) for geom, code in zip(gdf2.geometry, gdf2.land_code)],
            out_shape=(height, width),
            transform=transform,
            fill=0,
            dtype=np.int32
        )

        # å‘é‡åŒ–ç»Ÿè®¡å˜åŒ–ï¼ˆå¢åŠ ç©ºæ•°ç»„ä¿æŠ¤ï¼‰
        valid_mask = (raster1 != 0) & (raster2 != 0) & (raster1 != raster2)
        from_classes = raster1[valid_mask]
        to_classes = raster2[valid_mask]

        # æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º
        if from_classes.size == 0 or to_classes.size == 0:
            logger.warning(f"âš ï¸ æ—¶é—´æ®µ {i} åˆ° {i + 1} æ— æœ‰æ•ˆå˜åŒ–æ•°æ®")
            continue

        # ç¡®ä¿æ•°ç»„æ˜¯1ç»´çš„
        if from_classes.ndim > 1:
            from_classes = from_classes.flatten()
        if to_classes.ndim > 1:
            to_classes = to_classes.flatten()

        # ä½¿ç”¨np.uniqueå¿«é€Ÿç»Ÿè®¡å˜åŒ–å¯¹
        stacked = np.column_stack((from_classes, to_classes))
        pairs, counts = np.unique(stacked, axis=0, return_counts=True)

        for (from_class, to_class), count in zip(pairs, counts):
            key = (int(from_class), int(to_class))
            transition_counts[key] = transition_counts.get(key, 0) + count
            total_transitions += count

    # å¤„ç†æ— è½¬åŒ–æ•°æ®çš„æƒ…å†µ
    if total_transitions == 0:
        logger.warning("âš ï¸ æ‰€æœ‰æ—¶é—´æ®µå‡æ— åœ°ç±»è½¬åŒ–æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤é¢‘ç‡0")
        # è·å–æ‰€æœ‰åœ°ç±»ä»£ç 
        all_codes = set()
        for gdf in gdf_list:
            all_codes.update(gdf['land_code'].unique())
        # åˆ›å»ºé»˜è®¤è½¬åŒ–é¢‘ç‡ï¼ˆåŒç±»ä¸º1ï¼Œå¼‚ç±»ä¸º0ï¼‰
        t_matrix = []
        for i in all_codes:
            for j in all_codes:
                t_matrix.append({
                    'class_a': int(i),
                    'class_b': int(j),
                    'T': 1.0 if i == j else 0.0
                })
        return pd.DataFrame(t_matrix)

    # åˆ›å»ºè½¬åŒ–é¢‘ç‡çŸ©é˜µ
    t_matrix = [
        {'class_a': i, 'class_b': j, 'T': count / total_transitions}
        for (i, j), count in transition_counts.items()
    ]
    return validate_dataframe(pd.DataFrame(t_matrix), ['class_a', 'class_b', 'T'])

# è®¡ç®—è¾¹ç•Œæ¥è§¦å¯†åº¦ B(i,j)
def compute_boundary_density(gdf_list: List[gpd.GeoDataFrame]) -> pd.DataFrame:
    """
    è®¡ç®—æ‰€æœ‰å¹´ä»½ä¸­åœ°ç±»å¯¹çš„å¹³å‡è¾¹ç•Œæ¥è§¦å¯†åº¦ B(i,j)
    ä½¿ç”¨ç©ºé—´ç´¢å¼•åŠ é€Ÿè¾¹ç•Œç›¸äº¤è®¡ç®—
    """
    results = {}

    # è·å–æ‰€æœ‰åœ°ç±»ä»£ç 
    all_codes = set()
    for gdf in gdf_list:
        all_codes.update(gdf['land_code'].unique())

    # ç©ºæ•°æ®æ£€æŸ¥
    if not all_codes:
        logger.error("âŒ æ— æœ‰æ•ˆçš„åœ°ç±»æ•°æ®")
        return pd.DataFrame(columns=['class_a', 'class_b', 'B'])

    for gdf in gdf_list:
        # æŒ‰åœ°ç±»åˆ†ç»„
        grouped = gdf.groupby('land_code')
        class_codes = list(grouped.groups.keys())

        # è·³è¿‡ç©ºæ•°æ®
        if not class_codes:
            continue

        # ä½¿ç”¨unary_unionåˆå¹¶å‡ ä½•
        class_polygons = {code: unary_union(group.geometry) for code, group in grouped}

        # è®¡ç®—åœ°ç±»é—´æœ€å°è·ç¦»ï¼ˆæ›¿ä»£è¾¹ç•Œæ¥è§¦å¯†åº¦ï¼‰
        for i, code_i in enumerate(class_codes):
            geom_i = class_polygons[code_i]
            for j, code_j in enumerate(class_codes):
                if i == j:
                    continue
                geom_j = class_polygons[code_j]

                # è®¡ç®—æœ€å°è·ç¦»ï¼ˆè·ç¦»è¶Šå°æ¥è§¦å¯†åº¦è¶Šå¤§ï¼‰
                min_dist = geom_i.distance(geom_j)
                key = (code_i, code_j)
                # ä½¿ç”¨è·ç¦»çš„å€’æ•°ä½œä¸ºæ¥è§¦å¯†åº¦ï¼ˆè·ç¦»è¶Šå°å¯†åº¦è¶Šå¤§ï¼‰
                contact_density = 1 / (min_dist + 1)  # +1é¿å…é™¤é›¶
                results.setdefault(key, []).append(contact_density)

    # å¤„ç†æ— è¾¹ç•Œæ¥è§¦çš„æƒ…å†µ
    if not results:
        logger.warning("âš ï¸ æ— è¾¹ç•Œæ¥è§¦æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
        b_matrix = []
        for i in all_codes:
            for j in all_codes:
                if i != j:
                    b_matrix.append({
                        'class_a': int(i),
                        'class_b': int(j),
                        'B': 0.0
                    })
        return validate_dataframe(pd.DataFrame(b_matrix), ['class_a', 'class_b', 'B'])

    # æŒ‰åœ°ç±»å¯¹æ±‚å¹³å‡å€¼
    b_matrix = [
        {'class_a': int(i), 'class_b': int(j), 'B': np.mean(vals) if vals else 0}
        for (i, j), vals in results.items()
    ]
    return validate_dataframe(pd.DataFrame(b_matrix), ['class_a', 'class_b', 'B'])

# è®¡ç®—æ··åˆç¨‹åº¦æŒ‡æ•° M(i,j)ï¼ˆæ»‘åŠ¨çª—å£å…±ç°é¢‘ç‡ï¼‰
def compute_mixing_index(gdf_list: List[gpd.GeoDataFrame], resolution: int = 30, window_size: int = 3) -> pd.DataFrame:
    """
    æ»‘åŠ¨çª—å£æ³•è®¡ç®—æ··åˆç¨‹åº¦æŒ‡æ•° M(i,j)
    ä½¿ç”¨å‘é‡åŒ–çª—å£æ“ä½œæ›¿ä»£åŒé‡å¾ªç¯
    """
    mixing_counts = {}
    total_count = 0
    CHUNK_SIZE = 1000  # åˆ†å—å¤§å°

    # è·å–æ‰€æœ‰åœ°ç±»ä»£ç 
    all_codes = set()
    for gdf in gdf_list:
        all_codes.update(gdf['land_code'].unique())

    # ç©ºæ•°æ®æ£€æŸ¥
    if not all_codes:
        logger.error("âŒ æ— æœ‰æ•ˆçš„åœ°ç±»æ•°æ®")
        return pd.DataFrame(columns=['class_a', 'class_b', 'M'])

    for gdf in gdf_list:
        bounds = gdf.total_bounds
        transform = from_origin(bounds[0], bounds[3], resolution, resolution)
        width = int((bounds[2] - bounds[0]) / resolution) + 1
        height = int((bounds[3] - bounds[1]) / resolution) + 1

        # æ …æ ¼åŒ–ï¼ˆä½¿ç”¨æ•´å‹ç¼–ç ï¼‰
        raster = rasterize(
            [(geom, int(code)) for geom, code in zip(gdf.geometry, gdf.land_code)],
            out_shape=(height, width),
            transform=transform,
            fill=0,
            dtype=np.int16
        )

        # å®šä¹‰æ··åˆè®¡æ•°å‡½æ•°
        def count_mixing(window):
            center = window[window_size // 2, window_size // 2]
            if center == 0:
                return 0
            unique_vals = np.unique(window)
            # ç§»é™¤0å’Œä¸­å¿ƒå€¼
            unique_vals = unique_vals[(unique_vals != 0) & (unique_vals != center)]
            return len(unique_vals)

        # åˆ†å—å¤„ç†å¤§å‹æ …æ ¼
        for y in range(0, height, CHUNK_SIZE):
            y_end = min(y + CHUNK_SIZE, height)
            for x in range(0, width, CHUNK_SIZE):
                x_end = min(x + CHUNK_SIZE, width)

                # ä»…å¤„ç†åŒ…å«æœ‰æ•ˆæ•°æ®çš„åŒºå—
                chunk = raster[y:y_end, x:x_end]
                if np.all(chunk == 0):
                    continue

                # åº”ç”¨æ»‘åŠ¨çª—å£
                mixing_chunk = generic_filter(
                    chunk,
                    count_mixing,
                    size=(window_size, window_size),
                    mode='constant',
                    cval=0
                )

                # ç»Ÿè®¡å½“å‰åŒºå—
                for land_code in np.unique(chunk):
                    if land_code == 0:
                        continue
                    mask = (chunk == land_code)
                    count = np.sum(mixing_chunk[mask])
                    mixing_counts[land_code] = mixing_counts.get(land_code, 0) + count
                    total_count += count

                # åŠæ—¶é‡Šæ”¾å†…å­˜
                del mixing_chunk
                gc.collect()

    # å¤„ç†æ— æ··åˆæ•°æ®çš„æƒ…å†µ
    if total_count == 0:
        logger.warning("âš ï¸ æ— æ··åˆæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
        m_matrix = []
        for i in all_codes:
            for j in all_codes:
                if i != j:
                    m_matrix.append({
                        'class_a': int(i),
                        'class_b': int(j),
                        'M': 0.0
                    })
        return validate_dataframe(pd.DataFrame(m_matrix), ['class_a', 'class_b', 'M'])

    # è½¬æ¢ä¸ºåœ°ç±»å¯¹å½¢å¼
    total_m = sum(mixing_counts.values())
    m_matrix = []
    for code, count_val in mixing_counts.items():
        for other_code in mixing_counts.keys():
            if code != other_code:
                m_matrix.append({
                    'class_a': int(code),
                    'class_b': int(other_code),
                    'M': count_val / total_m
                })
    return validate_dataframe(pd.DataFrame(m_matrix), ['class_a', 'class_b', 'M'])

def save_lcsm_matrix(df: pd.DataFrame, value_col: str, output_path: Union[str, Path]) -> None:
    """
    ä¿å­˜ä¸ºè€¦åˆå¼ºåº¦çŸ©é˜µæ ¼å¼ï¼ˆå¯¹è§’çº¿ç½®1ï¼‰
    """
    output_path = Path(output_path)
    logger.info("ğŸ“ æ­£åœ¨è½¬æ¢ä¸ºè€¦åˆå¼ºåº¦çŸ©é˜µæ ¼å¼...")

    # åˆ›å»ºé€è§†è¡¨
    pivot_df = df.pivot_table(index='class_a', columns='class_b', values=value_col, fill_value=0)

    # ç¡®ä¿æ‰€æœ‰åœ°ç±»éƒ½åœ¨è¡Œåˆ—ä¸­
    all_codes = sorted(set(pivot_df.index).union(pivot_df.columns))
    pivot_df = pivot_df.reindex(index=all_codes, columns=all_codes, fill_value=0)

    # å¯¹è§’çº¿ç½®1ï¼ˆåŒç±»åœ°ç‰©å®Œå…¨ä½œç”¨ï¼‰
    np.fill_diagonal(pivot_df.values, 1.0)

    # éªŒè¯å¯¹è§’çº¿
    diag_values = pivot_df.values.diagonal()
    logger.info(f"âœ… çŸ©é˜µå¯¹è§’çº¿éªŒè¯: min={diag_values.min():.4f}, max={diag_values.max():.4f} (åº”å…¨ä¸º1.0)")

    pivot_df.to_csv(output_path)
    logger.info(f"ğŸ’¾ å·²ä¿å­˜åœ°ç±»è€¦åˆå¼ºåº¦çŸ©é˜µè‡³ {output_path}")

def _run_worker(func, args, queue):
    """å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆæ¨¡å—çº§ï¼Œç¡®ä¿å¯pickleï¼‰"""
    try:
        result = func(*args)
        queue.put(result)
    except Exception as e:
        queue.put(e)

def run_with_timeout(func, args, timeout=7200):
    """å®‰å…¨æ‰§è¡Œå‡½æ•° - æ¨¡å—çº§å‡½æ•°é¿å…pickleé—®é¢˜"""
    q = Queue()
    p = Process(target=_run_worker, args=(func, args, q))
    p.start()
    p.join(timeout)

    if p.is_alive():
        p.terminate()
        p.join()
        raise TimeoutError(f"âŒ› {func.__name__} è®¡ç®—è¶…æ—¶ ({timeout}s)")

    result = q.get()
    if isinstance(result, Exception):
        raise result
    return result

@timeit("æ„å»ºåœ°ç±»è€¦åˆå¼ºåº¦å›¾è°± LCSM")
def build_lcsm(input_paths: List[Union[str, Path]], output_dir: Union[str, Path], resolution: int = 30) -> None:
    log_memory_usage()
    # ç»Ÿä¸€è·¯å¾„ç®¡ç†
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info("ğŸ“¥ åŠ è½½å¤šæœŸ features å›¾å±‚...")

    def check_memory(min_avail_gb=2.0):
        mem = psutil.virtual_memory()
        avail_gb = mem.available / 1024 ** 3
        if avail_gb < min_avail_gb:
            logger.warning(f"ğŸ›‘ å¯ç”¨å†…å­˜ä½äº{min_avail_gb}GB ({avail_gb:.2f}GB)ï¼Œè§¦å‘GCå›æ”¶")
            gc.collect()
            new_mem = psutil.virtual_memory()
            logger.info(f"â™»ï¸ GCåå¯ç”¨å†…å­˜: {new_mem.available / 1024 ** 3:.2f}GB")
        return mem.available

    # å¢é‡åŠ è½½å¤šæœŸæ•°æ®
    gdf_list = []
    for path in input_paths:
        try:
            # å†…å­˜è­¦æˆ’çº¿æ£€æŸ¥
            check_memory(1.5)

            gdf = read_geodata(str(path))

            # å­—æ®µåå…¼å®¹æ€§å¤„ç†
            land_code_col = find_column_by_normalized(gdf, "land_code")
            if not land_code_col:
                logger.error(f"âŒ æ–‡ä»¶ {path} ç¼ºå°‘ land_code å­—æ®µï¼Œè·³è¿‡å¤„ç†")
                continue
            if land_code_col != "land_code":
                gdf = gdf.rename(columns={land_code_col: "land_code"})

            # å¼ºåˆ¶è½¬æ¢ land_code ä¸ºæ•´æ•°
            gdf['land_code'] = gdf['land_code'].astype(int)

            # åŠ¨æ€ä¿®å¤é‡å¿ƒåæ ‡å­—æ®µ
            centroid_x_col = find_column_by_normalized(gdf, "centroid_x")
            centroid_y_col = find_column_by_normalized(gdf, "centroid_y")
            if not centroid_x_col or not centroid_y_col:
                logger.warning(f"âš ï¸ æ–‡ä»¶ {path} ç¼ºå°‘é‡å¿ƒåæ ‡å­—æ®µï¼Œè‡ªåŠ¨è®¡ç®—...")
                gdf['centroid_x'] = gdf.geometry.centroid.x
                gdf['centroid_y'] = gdf.geometry.centroid.y
            else:
                if centroid_x_col != "centroid_x":
                    gdf = gdf.rename(columns={centroid_x_col: "centroid_x"})
                if centroid_y_col != "centroid_y":
                    gdf = gdf.rename(columns={centroid_y_col: "centroid_y"})

            gdf_list.append(gdf)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {path} | é”™è¯¯: {str(e)}")
            continue

    # ç©ºæ•°æ®æ£€æŸ¥
    if not gdf_list:
        logger.critical("âŒ æ— æœ‰æ•ˆè¾“å…¥æ•°æ®ï¼Œç»ˆæ­¢å¤„ç†")
        return

    # åŠ è½½é…ç½®å‚æ•°
    config = load_weights()
    weights_dict = config.get('lcsm_weights', {
        "transition_frequency": 0.5,
        "contact_density": 0.3,
        "mixture_index": 0.2
    })
    min_dist, max_dist = config.get('lcsm_distance_range', [10, 300])
    decay_dict = config.get('distance_decay', {'a': 1000, 'b': 0.5})
    decay_a = decay_dict.get('a', 1000)
    decay_b = decay_dict.get('b', 0.5)
    impact_decay_factor = config.get('impact_decay_factor', 0.01)

    # è¶…æ—¶å®‰å…¨è®¡ç®—æŒ‡æ ‡
    try:
        logger.info("â³ å¼€å§‹è®¡ç®—è½¬åŒ–é¢‘ç‡æŒ‡æ ‡ (T)...")
        check_memory(1.5)
        df_t = run_with_timeout(compute_transition_frequency, (gdf_list, resolution), 7200)

        logger.info("â³ å¼€å§‹è®¡ç®—è¾¹ç•Œå¯†åº¦æŒ‡æ ‡ (B)...")
        check_memory(1.5)
        df_b = run_with_timeout(compute_boundary_density, (gdf_list,), 7200)

        logger.info("â³ å¼€å§‹è®¡ç®—æ··åˆæŒ‡æ•°æŒ‡æ ‡ (M)...")
        check_memory(1.5)
        df_m = run_with_timeout(compute_mixing_index, (gdf_list, resolution), 7200)
    except TimeoutError as e:
        logger.critical(f"âŒ {str(e)}ï¼Œç»ˆæ­¢è®¡ç®—")
        return
    except Exception as e:
        logger.critical(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}")
        return

    # åˆå¹¶æŒ‡æ ‡ï¼ˆä½¿ç”¨outer joinå¹¶å¡«å……0ï¼‰
    logger.info("ğŸ”— åˆå¹¶æŒ‡æ ‡è¡¨...")
    df = pd.merge(df_t, df_b, on=['class_a', 'class_b'], how='outer', suffixes=('', '_b'))
    df = pd.merge(df, df_m, on=['class_a', 'class_b'], how='outer', suffixes=('', '_m'))

    # é‡å‘½ååˆ—ï¼ˆé¿å…åç¼€å†²çªï¼‰
    col_mapping = {'T': 'T', 'B': 'B', 'M': 'M'}
    df = df.rename(columns=col_mapping)

    # å¡«å……NaNå€¼
    df.fillna({'T': 0, 'B': 0, 'M': 0}, inplace=True)

    # æ·»åŠ åŒç±»åœ°ç‰©å¯¹
    logger.info("â• æ·»åŠ åŒç±»åœ°ç‰©å¯¹...")
    all_codes = set()
    for df_part in [df_t, df_b, df_m]:
        all_codes.update(df_part['class_a'].unique())
        all_codes.update(df_part['class_b'].unique())

    same_class_pairs = []
    for code in all_codes:
        if not ((df['class_a'] == code) & (df['class_b'] == code)).any():
            same_class_pairs.append({
                'class_a': code, 'class_b': code,
                'T': 1.0, 'B': 1.0, 'M': 1.0
            })

    if same_class_pairs:
        df = pd.concat([df, pd.DataFrame(same_class_pairs)], ignore_index=True)

    # å½’ä¸€åŒ–å¤„ç†
    logger.info("ğŸ“Š æ‰§è¡ŒæŒ‡æ ‡å½’ä¸€åŒ–...")
    for col in ['T', 'B', 'M']:
        min_val = df[col].min()
        max_val = df[col].max()
        range_val = max_val - min_val
        if range_val < 1e-6:
            logger.warning(f"âš ï¸ æŒ‡æ ‡ {col} ç¼ºä¹å˜åŒ–ï¼Œå½’ä¸€åŒ–è®¾ç½®ä¸º0.5")
            df[f"{col}_norm"] = 0.5
        else:
            df[f"{col}_norm"] = (df[col] - min_val) / range_val

    # åŠ æƒåˆæˆè€¦åˆå¼ºåº¦
    logger.info("âš–ï¸ è®¡ç®—åŠ æƒè€¦åˆå¼ºåº¦ (LCS_norm)...")
    df["LCS_norm"] = (
            weights_dict["transition_frequency"] * df["T_norm"] +
            weights_dict["contact_density"] * df["B_norm"] +
            weights_dict["mixture_index"] * df["M_norm"]
    )

    # æ ¸å¿ƒè½¬æ¢é€»è¾‘
    logger.info("âš™ï¸ è®¡ç®—ç‰©ç†è·ç¦» (LCS_distance)...")
    df["LCS_distance"] = max_dist - df["LCS_norm"] * (max_dist - min_dist)

    logger.info("ğŸ§ª è½¬æ¢ä½œç”¨å¼ºåº¦ (impact_strength)...")
    df["impact_strength"] = decay_a * np.power(df["LCS_distance"] + 1e-6, -decay_b)
    df["impact_strength_exp"] = np.exp(-impact_decay_factor * df["LCS_distance"])

    # åŒç±»ç‰¹æ®Šå¤„ç†
    same_class_mask = (df['class_a'] == df['class_b'])
    df.loc[same_class_mask, 'LCS_distance'] = 0
    df.loc[same_class_mask, 'impact_strength'] = 1.0
    df.loc[same_class_mask, 'impact_strength_exp'] = 1.0

    # ä¿å­˜ç»“æœ
    pair_path = output_dir / "lcsm_pairs.csv"
    matrix_path = output_dir / "lcsm_matrix.csv"
    df.to_csv(pair_path, index=False)
    logger.info(f"ğŸ’¾ å·²ä¿å­˜LCSMå…³ç³»å¯¹è¡¨: {pair_path}")

    save_lcsm_matrix(df, "impact_strength", matrix_path)
    distance_matrix_path = output_dir / "lcsm_distance_matrix.csv"
    save_lcsm_matrix(df, "LCS_distance", distance_matrix_path)
    logger.info(f"ğŸ“ å·²ä¿å­˜è·ç¦»çŸ©é˜µè‡³ {distance_matrix_path}")
    log_memory_usage()