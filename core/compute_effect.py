import pandas as pd
import geopandas as gpd
import numpy as np
import os
import math
import json
import logging
from tqdm import tqdm
from rtree import index
from utils.geodata_io import read_geodata, write_geodata
from utils.timer import timeit
from shapely.geometry import Polygon
from shapely.strtree import STRtree
from concurrent.futures import ProcessPoolExecutor, as_completed
import gc
import psutil
import time

logger = logging.getLogger(__name__)

@timeit("åŠ è½½æƒé‡é…ç½®")
def load_decay_params(config_path="config/weight_config.json"):
    """åŠ è½½è·ç¦»è¡°å‡å‚æ•°å¹¶æ·»åŠ å¼‚å¸¸å¤„ç†"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            a = config['distance_decay']['a']
            b = config['distance_decay']['b']
            logger.info(f"âœ… åŠ è½½è·ç¦»è¡°å‡å‚æ•°: a={a}, b={b} (å…¬å¼: buffer_distance = a * exp(-b * strength))")
            return a, b
    except Exception as e:
        logger.critical(f"âŒ è¡°å‡å‚æ•°åŠ è½½å¤±è´¥: {str(e)}")
        return 1000, 0.5  # é»˜è®¤å€¼

def _process_batch(args):
    """ç‹¬ç«‹å‡½æ•°ç¡®ä¿å¯åºåˆ—åŒ–ï¼ˆä½¿ç”¨LAIMçš„LAIå€¼ï¼‰"""
    batch_records, features, adj_df, laim_dict, decay_a, decay_b = args
    results = []
    for _, row in batch_records.iterrows():
        a_id, b_id = row['poly_id_a'], row['poly_id_b']
        a_code, b_code = row['land_code_a'], row['land_code_b']

        try:
            # è·å–å‡ ä½•
            geom_a = features.loc[a_id].geometry
            geom_b = features.loc[b_id].geometry

            # ä½¿ç”¨LAIMçš„LAIå€¼ï¼ˆé‚»æ¥å½±å“å€¼ï¼‰
            lai = laim_dict.get((a_code, b_code)) or laim_dict.get((b_code, a_code))
            if not lai:
                continue

            # è®¡ç®—ç¼“å†²è·ç¦»ï¼ˆä½¿ç”¨LAIå€¼ï¼‰
            buffer_distance = decay_a * math.exp(-decay_b * lai)

            # å‡ ä½•æœ‰æ•ˆæ€§æ£€æŸ¥
            if not geom_a.is_valid or not geom_b.is_valid:
                continue

            # åŠ¨æ€è®¡ç®—ç¼“å†²åŒºåˆ†è¾¨ç‡
            area = max(geom_a.area, geom_b.area)
            resolution = 16 if area > 1e6 else 8

            # æ·»åŠ ç¼“å†²åŒºå®¹å·®å‡å°‘è®¡ç®—å¤æ‚åº¦
            buffer_a = geom_a.buffer(buffer_distance, resolution=resolution, join_style=2)
            inter = buffer_a.intersection(geom_b)

            if inter.is_empty or inter.area < 1e-6:
                continue

            results.append({
                "source_id": a_id,
                "target_id": b_id,
                "source_code": a_code,
                "target_code": b_code,
                "geometry": inter,
                "buffer_distance": buffer_distance,
                "impact_strength": lai  # ä½¿ç”¨LAIå€¼
            })
        except Exception as e:
            logger.error(f"âŒ å¤„ç†é‚»æ¥å¯¹å¤±è´¥ {a_id}-{b_id}: {str(e)}")
    return results

@timeit("ç”Ÿæˆç›´æ¥é‚»æ¥ä½œç”¨å›¾å±‚ DAL")
def generate_direct_effect_layer(features_path, adjacency_csv, laim_csv, output_path):
    """
    ç”Ÿæˆç›´æ¥é‚»æ¥ä½œç”¨å›¾å±‚ä¼˜åŒ–ï¼š
    1. å¢å¼ºå‡ ä½•æœ‰æ•ˆæ€§æ£€æŸ¥
    2. ä¼˜åŒ–ç¼“å†²åŒºåˆ†è¾¨ç‡è®¾ç½®
    3. æ”¹è¿›å¤šè¿›ç¨‹ä»»åŠ¡è°ƒåº¦
    """
    # è¯»å–æ•°æ®
    features = read_geodata(features_path).set_index("poly_id")
    adjacency_df = pd.read_csv(adjacency_csv)
    laim_df = pd.read_csv(laim_csv)  # ä½¿ç”¨LAIMæ•°æ®

    # æ„å»ºLAIMçš„LAIå­—å…¸
    laim_dict = {}
    for _, row in laim_df.iterrows():
        key = (row["class_a"], row["class_b"])
        laim_dict[key] = row["LAI"]  # ä½¿ç”¨LAIå€¼

    # è·å–è¡°å‡å‚æ•°
    decay_a, decay_b = load_decay_params()

    # å†…å­˜ç›‘æ§å‡½æ•°
    def check_memory(min_avail_gb=1.0):
        mem = psutil.virtual_memory()
        avail_gb = mem.available / 1024 ** 3
        if avail_gb < min_avail_gb:
            logger.warning(f"ğŸ›‘ å¯ç”¨å†…å­˜ä½äº{min_avail_gb}GB ({avail_gb:.2f}GB)ï¼Œè§¦å‘GCå›æ”¶")
            gc.collect()
            time.sleep(1)
            new_mem = psutil.virtual_memory()
            logger.info(f"â™»ï¸ GCåå¯ç”¨å†…å­˜: {new_mem.available / 1024 ** 3:.2f}GB")
        return mem.available

    # åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°
    total_records = len(adjacency_df)
    avail_mem = psutil.virtual_memory().available / (1024 ** 3)
    chunk_size = max(100, min(5000, total_records // max(1, int(avail_mem / 0.5))))

    logger.info(f"ğŸ§  åŠ¨æ€åˆ†å— | è®°å½•æ•°: {total_records} | å¯ç”¨å†…å­˜: {avail_mem:.1f}GB â†’ åˆ†å—å¤§å°: {chunk_size}")

    # å¤šè¿›ç¨‹å¤„ç†
    results = []
    cpu_count = os.cpu_count() or 4
    logger.info(f"ğŸš€ å¯åŠ¨{cpu_count}è¿›ç¨‹è®¡ç®—ç›´æ¥é‚»æ¥ä½œç”¨...")

    with ProcessPoolExecutor(max_workers=cpu_count) as executor:
        futures = []
        for i in range(0, total_records, chunk_size):
            # å†…å­˜è­¦æˆ’æ£€æŸ¥
            check_memory(1.5)

            chunk = adjacency_df.iloc[i:i + chunk_size]
            futures.append(
                executor.submit(
                    _process_batch,
                    (chunk, features, adjacency_df, laim_dict, decay_a, decay_b)
                )
            )

        # è¿›åº¦ç›‘æ§
        with tqdm(total=len(futures), desc="å¤„ç†é‚»æ¥å¯¹") as pbar:
            for future in as_completed(futures):
                try:
                    batch_results = future.result()
                    if batch_results:
                        results.extend(batch_results)
                except Exception as e:
                    logger.error(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                finally:
                    pbar.update(1)
                    # åŠæ—¶é‡Šæ”¾å†…å­˜
                    del future
                    gc.collect()

    # ä¿å­˜ç»“æœ
    if results:
        gdf = gpd.GeoDataFrame(results, crs=features.crs)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        write_geodata(gdf, output_path)
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ DAL å›¾å±‚: {output_path} (è¦ç´ æ•°: {len(gdf)})")
        return output_path
    else:
        logger.warning("âš ï¸ æ— æœ‰æ•ˆç›¸äº¤å›¾å½¢ï¼Œæœªç”Ÿæˆ DAL å›¾å±‚")
        return None

def _process_indirect_chunk(args):
    """ç‹¬ç«‹å‡½æ•°å¤„ç†é—´æ¥é‚»æ¥å—"""
    chunk_indices, gdf, spatial_idx, lcsm_dict, decay_a, decay_b = args
    chunk_results = []
    for idx_a in chunk_indices:
        row_a = gdf.iloc[idx_a]
        geom_a = row_a.geometry
        a_code = row_a['land_code']

        # ç©ºé—´æŸ¥è¯¢å€™é€‰é›†
        candidate_indices = spatial_idx.query(geom_a, predicate='intersects')
        for idx_b in candidate_indices:
            if idx_b == idx_a:
                continue

            row_b = gdf.iloc[idx_b]
            b_code = row_b['land_code']

            # è·³è¿‡åŒç±»åœ°ç‰©
            if a_code == b_code:
                continue

            # ä½¿ç”¨LCSMçš„ä½œç”¨å¼ºåº¦
            strength = lcsm_dict.get((a_code, b_code), 0.01)

            # è·ç¦»è¡°å‡è®¡ç®—
            buffer_distance = decay_a * math.exp(-decay_b * strength)

            # ç®€åŒ–å‡ ä½•æ“ä½œ
            try:
                buffer_a = geom_a.buffer(
                    buffer_distance,
                    resolution=8,  # å›ºå®šåˆ†è¾¨ç‡å‡å°‘è®¡ç®—é‡
                    join_style=2    # æ–œæ¥è¿æ¥å‡å°‘å¼‚å¸¸
                )
                inter = buffer_a.intersection(row_b.geometry)

                if not inter.is_empty and inter.area > (geom_a.area * 0.001):
                    chunk_results.append({
                        "source_id": row_a['poly_id'],
                        "target_id": row_b['poly_id'],
                        "source_code": a_code,
                        "target_code": b_code,
                        "geometry": inter,
                        "buffer_distance": buffer_distance,
                        "impact_strength": strength
                    })
            except Exception as e:
                logger.error(f"å‡ ä½•æ“ä½œå¤±è´¥: {str(e)}")
    return chunk_results

@timeit("ç”Ÿæˆé—´æ¥é‚»æ¥ä½œç”¨å›¾å±‚ IAL")
def generate_indirect_effect_layer(features_path, laim_csv, lcsm_csv, output_path):
    """
    ç”Ÿæˆé—´æ¥é‚»æ¥ä½œç”¨å›¾å±‚ä¼˜åŒ–ï¼š
    1. ä½¿ç”¨LCSMçš„ä½œç”¨å¼ºåº¦å€¼ï¼ˆimpact_strengthï¼‰
    2. ä¼˜åŒ–ç©ºé—´ç´¢å¼•æŸ¥è¯¢æ•ˆç‡
    3. æ”¹è¿›åˆ†å—å¹¶è¡Œç­–ç•¥
    """
    # 1. åŠ è½½å‚æ•°å’Œæ•°æ®é›†
    decay_a, decay_b = load_decay_params()
    gdf = read_geodata(features_path)
    total_features = len(gdf)
    logger.info(f"ğŸ“¥ åŠ è½½ç‰¹å¾æ•°æ®: {features_path} â†’ {total_features}ä¸ªè¦ç´ ")

    # 2. åŠ è½½LCSMçŸ©é˜µï¼ˆä½¿ç”¨impact_strengthå­—æ®µï¼‰
    lcsm_df = pd.read_csv(lcsm_csv)
    lcsm_dict = {}
    for _, row in lcsm_df.iterrows():
        key = (row['class_a'], row['class_b'])
        lcsm_dict[key] = row['impact_strength']

    # 3. æ„å»ºç©ºé—´ç´¢å¼•
    logger.info("ğŸ” æ„å»ºç©ºé—´ç´¢å¼•...")
    spatial_idx = STRtree(gdf.geometry)

    # å†…å­˜ç›‘æ§å‡½æ•°
    def check_memory(min_avail_gb=1.0):
        mem = psutil.virtual_memory()
        if mem.available < min_avail_gb * 1024 ** 3:
            gc.collect()
            time.sleep(1)
            return psutil.virtual_memory().available
        return mem.available

    # 4. åŠ¨æ€åˆ†å—å¹¶è¡Œè®¡ç®—
    logger.info("ğŸš€ å¯åŠ¨å¹¶è¡Œè®¡ç®—é—´æ¥ä½œç”¨...")
    cpu_count = os.cpu_count() or 4

    # åŸºäºå†…å­˜çš„åŠ¨æ€åˆ†å—
    avail_mem = psutil.virtual_memory().available / (1024 ** 3)
    chunk_size = max(50, min(500, total_features // max(1, int(avail_mem / 0.5))))

    logger.info(f"ğŸ§  åŠ¨æ€åˆ†å— | è¦ç´ æ•°: {total_features} | å¯ç”¨å†…å­˜: {avail_mem:.1f}GB â†’ åˆ†å—å¤§å°: {chunk_size}")

    indices = list(range(total_features))
    results = []

    with ProcessPoolExecutor(max_workers=cpu_count) as executor:
        futures = []
        for i in range(0, total_features, chunk_size):
            # å†…å­˜è­¦æˆ’æ£€æŸ¥
            check_memory(1.5)

            chunk = indices[i:i + chunk_size]
            futures.append(executor.submit(
                _process_indirect_chunk,
                (chunk, gdf, spatial_idx, lcsm_dict, decay_a, decay_b)
            ))

        # è¿›åº¦ç›‘æ§
        with tqdm(total=len(futures), desc="å¤„ç†é—´æ¥é‚»æ¥") as pbar:
            for future in as_completed(futures):
                try:
                    chunk_results = future.result()
                    if chunk_results:
                        results.extend(chunk_results)
                except Exception as e:
                    logger.error(f"âŒ åˆ†å—å¤„ç†å¤±è´¥: {str(e)}")
                finally:
                    pbar.update(1)
                    # åŠæ—¶é‡Šæ”¾å†…å­˜
                    del future
                    gc.collect()

    # 5. ä¿å­˜ç»“æœ
    if results:
        gdf_out = gpd.GeoDataFrame(results, crs=gdf.crs)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        write_geodata(gdf_out, output_path)
        logger.info(f"âœ… ç”ŸæˆIALå›¾å±‚: {output_path} (è¦ç´ æ•°: {len(gdf_out)})")
        return output_path
    else:
        logger.warning("âš ï¸ æ— æœ‰æ•ˆé—´æ¥ä½œç”¨åŒºåŸŸ")
        return None

@timeit("ä½œç”¨å¼ºåº¦å­—æ®µèµ‹å€¼")
def assign_effect_strength(effect_path, lcsm_csv, output_path, strength_field="impact_strength"):
    """
    1. å‘é‡åŒ–æ“ä½œæ›¿ä»£apply
    2. å‡å°‘å†…å­˜æ‹·è´
    """
    gdf = read_geodata(effect_path)
    lcsm_df = pd.read_csv(lcsm_csv)

    # å‘é‡åŒ–æ„å»ºå­—å…¸
    lcsm_dict = {}
    for _, row in lcsm_df.iterrows():
        key1 = (row['class_a'], row['class_b'])
        key2 = (row['class_b'], row['class_a'])
        lcsm_dict[key1] = row['impact_strength']
        lcsm_dict[key2] = row['impact_strength']

    # å‘é‡åŒ–èµ‹å€¼æ›¿ä»£apply
    # åˆ›å»ºä¸´æ—¶åˆ—å­˜å‚¨æŸ¥è¯¢é”®
    gdf['strength_key'] = list(zip(gdf['source_code'], gdf['target_code']))

    # ä½¿ç”¨mapæ–¹æ³•å‘é‡åŒ–èµ‹å€¼
    gdf[strength_field] = gdf['strength_key'].map(
        lambda x: lcsm_dict.get(x, 0.01)
    )

    # åˆ é™¤ä¸´æ—¶åˆ—
    del gdf['strength_key']

    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    write_geodata(gdf, output_path)
    logger.info(f"âœ… {strength_field} èµ‹å€¼å®Œæˆ: {output_path}")
    return output_path
