import os
import sys
from pathlib import Path
import logging
import json
import time
import traceback
import pandas as pd
import geopandas as gpd
import numpy as np
from tqdm import tqdm
import psutil
import gc
import multiprocessing as mp
import threading
from datetime import datetime
from utils.timer import timeit
from utils.config_reader import load_config
from core.preprocess import preprocess_shapefile
from core.classify_landuse import classify_by_landuse, load_class_map
from core.extract_features import extract_features
from core.build_laim import build_laim
from core.build_lcsm import build_lcsm
from core.adjacency_detector import detect_adjacency_pairs
from core.compute_effect import (
    generate_direct_effect_layer,
    generate_indirect_effect_layer,
    assign_effect_strength
)
from core.merge_and_raster import rasterize_lai, crop_raster, merge_effect_layers

logger = logging.getLogger(__name__)

def log_memory_usage():
    """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    mem = psutil.virtual_memory()
    logger.info(
        f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {mem.used / 1024 ** 3:.2f}GB/{mem.total / 1024 ** 3:.2f}GB (å¯ç”¨: {mem.available / 1024 ** 3:.2f}GB)")

# è¯»å–å‚æ•°
config = load_config()
INPUT_ROOT = Path(config["input_root"])
OUTPUT_ROOT = Path(config["output_root"])
YEARS = config["years"]
LAIM_YEARS = config["laim_years"]
LCSM_YEARS = config["lcsm_years"]
TARGET_YEAR = str(config["target_year"])
RESOLUTION = config["resolution"]

# é¢„åˆ›å»ºæ‰€æœ‰è¾“å‡ºç›®å½•
output_dirs = ["standardized", "classified", "features",
               "laim", "lcsm", "adjacency", "effect", "final", "logs"]
for d in output_dirs:
    os.makedirs(f"{OUTPUT_ROOT}/{d}", exist_ok=True)

# åŠ è½½åœ°ç±»ç¼–ç æ˜ å°„
class_map = load_class_map()

# å­˜å‚¨åˆ†ç±»å›¾å±‚å’Œ features è·¯å¾„ï¼ˆä¾›åç»­ LAIM å’Œ LCSM æ„å»ºï¼‰
classified_dict = {}
features_dict = {}

@timeit("æ•°æ®é¢„å¤„ç†ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")
def preprocess_all_years():
    """S1ï¼šæ ‡å‡†åŒ–å¤„ç†æµæ°´çº¿ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰"""
    logger.info("ğŸ”„ å¼€å§‹æ‰€æœ‰å¹´ä»½çš„æ ‡å‡†åŒ–å¤„ç†...")
    # ä»é…ç½®è¯»å–å›¾å±‚å‰ç¼€
    LAYER_PREFIX = config.get("gpkg_layer_prefix", "xfdnlanduse_")
    total_time = 0

    for year in tqdm(YEARS, desc="ğŸ“… å¤„ç†å¹´ä»½"):
        y = str(year)
        start_time = time.time()

        # æ„å»ºå®Œæ•´å›¾å±‚è·¯å¾„
        layer_name = f"{LAYER_PREFIX}{y}"
        input_path = f"{INPUT_ROOT}|layername={layer_name}"

        # æ ‡å‡†åŒ–è¾“å‡ºè·¯å¾„
        std_dir = OUTPUT_ROOT / "standardized" / y
        std_dir.mkdir(exist_ok=True)
        std_path = std_dir / f"landuse_{y}.gpkg"

        try:
            # é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–+ä¿®å¤ï¼‰
            preprocess_shapefile(input_path, str(std_path))
            logger.info(f"âœ… {y}å¹´æ ‡å‡†åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ {y}å¹´æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
            continue

        year_time = time.time() - start_time
        total_time += year_time
        logger.info(f"â±ï¸ {y}å¹´é¢„å¤„ç†ç”¨æ—¶: {round(year_time, 2)}ç§’")

    logger.info(f"âœ… æ‰€æœ‰å¹´ä»½é¢„å¤„ç†å®Œæˆ! æ€»ç”¨æ—¶: {round(total_time, 2)}ç§’")
    return total_time

@timeit("ç‰¹å¾æå–ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")
def extract_features_all_years():
    """S2ï¼šåˆ†ç±»+ç‰¹å¾æå–æµæ°´çº¿ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰"""
    logger.info("ğŸ”„ å¼€å§‹æ‰€æœ‰å¹´ä»½çš„åˆ†ç±»ä¸ç‰¹å¾æå–...")
    total_time = 0

    for year in tqdm(YEARS, desc="ğŸ“… å¤„ç†å¹´ä»½"):
        y = str(year)
        start_time = time.time()

        # æ ‡å‡†åŒ–æ•°æ®è·¯å¾„
        std_path = OUTPUT_ROOT / "standardized" / y / f"landuse_{y}.gpkg"
        # åˆ†ç±»è¾“å‡ºè·¯å¾„
        class_dir = OUTPUT_ROOT / "classified" / y
        # ç‰¹å¾è¾“å‡ºè·¯å¾„
        feat_dir = OUTPUT_ROOT / "features" / y
        feat_dir.mkdir(exist_ok=True)
        feat_path = feat_dir / f"features_{y}.gpkg"

        try:
            # åˆ†ç±»ï¼ˆä½¿ç”¨æ˜¾å¼å›¾å±‚åï¼‰
            classify_by_landuse(str(std_path), str(class_dir), class_map)
            # ç‰¹å¾æå–
            extract_features(str(std_path), str(feat_path))
            logger.info(f"âœ… {y}å¹´åˆ†ç±»ä¸ç‰¹å¾æå–æˆåŠŸ")
            # å­˜å‚¨è·¯å¾„å¼•ç”¨
            classified_dict[y] = class_dir
            features_dict[y] = feat_path
        except Exception as e:
            logger.error(f"âŒ {y}å¹´åˆ†ç±»ä¸ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            continue

        year_time = time.time() - start_time
        total_time += year_time
        logger.info(f"â±ï¸ {y}å¹´ç‰¹å¾æå–ç”¨æ—¶: {round(year_time, 2)}ç§’")

    logger.info(f"âœ… æ‰€æœ‰å¹´ä»½ç‰¹å¾æå–å®Œæˆ! æ€»ç”¨æ—¶: {round(total_time, 2)}ç§’")
    return total_time

def main():
    all_times = []
    stage_logs = []
    start_time = time.time()

    timeout_sec = 18000
    timeout_event = threading.Event()

    def timeout_handler():
        """è¶…æ—¶å¤„ç†å‡½æ•°"""
        timeout_event.set()
        raise TimeoutError("å…¨å±€è®¡ç®—è¶…æ—¶")

    # åˆ›å»ºå®šæ—¶å™¨ä½†ä¸ç«‹å³å¯åŠ¨
    timer = threading.Timer(timeout_sec, timeout_handler)

    # é¢„åŠ è½½åœ°ç±»æ˜ å°„è¡¨ï¼ˆç¡®ä¿åªåŠ è½½ä¸€æ¬¡ï¼‰
    class_map = load_class_map()

    try:
        timer.start()  # å¯åŠ¨è¶…æ—¶è®¡æ—¶å™¨

        # ======= ç¯èŠ‚1: æ•°æ®é¢„å¤„ç†ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰ =======
        stage_start = time.time()
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®é¢„å¤„ç†ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰...")
        preprocess_time = preprocess_all_years()
        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "data_preprocessing", "time_sec": stage_time})
        logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ! æ€»ç”¨æ—¶: {stage_time}ç§’")

        # ======= ç¯èŠ‚2: ç‰¹å¾æå–ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰ =======
        stage_start = time.time()
        logger.info("ğŸ” å¼€å§‹åœ°ç±»åˆ†ç±»ä¸ç‰¹å¾æå–ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰...")
        feature_time = extract_features_all_years()
        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "feature_extraction", "time_sec": stage_time})
        logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ! æ€»ç”¨æ—¶: {stage_time}ç§’")

        # ======= ç¯èŠ‚3: çŸ¥è¯†å›¾è°±æ„å»º =======
        stage_start = time.time()
        logger.info("ğŸ§  æ„å»ºçŸ¥è¯†å›¾è°±...")

        # æ„å»ºLAIMå›¾è°±
        logger.info("ğŸ“Š æ„å»º LAIM å›¾è°±ï¼ˆé‚»æ¥å½±å“ï¼‰...")
        laim_classified_paths = [str(classified_dict[str(y)]) for y in LAIM_YEARS]
        build_laim(laim_classified_paths, str(OUTPUT_ROOT / "laim"))

        # æ„å»ºLCSMå›¾è°±
        logger.info("ğŸ“Š æ„å»º LCSM å›¾è°±ï¼ˆè€¦åˆå¼ºåº¦ï¼‰...")
        lcsm_feature_paths = [str(features_dict[str(y)]) for y in LCSM_YEARS]
        build_lcsm(lcsm_feature_paths, str(OUTPUT_ROOT / "lcsm"))

        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "knowledge_graph", "time_sec": stage_time})
        logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ! ç”¨æ—¶: {stage_time}ç§’")

        # ======= ç¯èŠ‚4: é‚»æ¥å…³ç³»è¯†åˆ« =======
        stage_start = time.time()
        logger.info("ğŸ” è¯†åˆ«é‚»æ¥å…³ç³»...")
        # åŠ¨æ€è·¯å¾„æ„å»º
        feat_path = features_dict[TARGET_YEAR]
        adj_dir = OUTPUT_ROOT / "adjacency"
        adj_dir.mkdir(exist_ok=True)
        adj_path = adj_dir / f"{TARGET_YEAR}_adjacency.csv"

        # æ‰§è¡Œé‚»æ¥å…³ç³»æ£€æµ‹
        detect_adjacency_pairs(str(feat_path), str(adj_path))

        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "adjacency_detection", "time_sec": stage_time})
        logger.info(f"âœ… é‚»æ¥å…³ç³»è¯†åˆ«å®Œæˆ! ç”¨æ—¶: {stage_time}ç§’")

        # ======= ç¯èŠ‚5: é‚»æ¥ä½œç”¨è®¡ç®— =======
        stage_start = time.time()
        logger.info("ğŸ§® è®¡ç®—é‚»æ¥ä½œç”¨...")
        # å‡†å¤‡å…¶ä»–è·¯å¾„
        std_path = OUTPUT_ROOT / "standardized" / TARGET_YEAR / f"landuse_{TARGET_YEAR}.gpkg"
        effect_dir = OUTPUT_ROOT / "effect"
        effect_dir.mkdir(exist_ok=True)
        dal_raw = effect_dir / f"{TARGET_YEAR}_dal_raw.gpkg"
        dal_final = effect_dir / f"{TARGET_YEAR}_dal.gpkg"
        ial_raw = effect_dir / f"{TARGET_YEAR}_ial_raw.gpkg"
        ial_final = effect_dir / f"{TARGET_YEAR}_ial.gpkg"
        final_dir = OUTPUT_ROOT / "final"
        final_dir.mkdir(exist_ok=True)
        merged_path = final_dir / f"{TARGET_YEAR}_linjie.gpkg"
        laim_path = OUTPUT_ROOT / "laim" / "laim_pairs.csv"
        lcsm_path = OUTPUT_ROOT / "lcsm" / "lcsm_pairs.csv"

        # æ‰§è¡Œè®¡ç®—é“¾
        generate_direct_effect_layer(str(feat_path), str(adj_path), str(laim_path), str(dal_raw))
        assign_effect_strength(str(dal_raw), str(lcsm_path), str(dal_final), "DAL_Strength")
        generate_indirect_effect_layer(str(feat_path), str(laim_path), str(lcsm_path), str(ial_raw))
        assign_effect_strength(str(ial_raw), str(lcsm_path), str(ial_final), "IAL_Strength")
        merge_effect_layers(str(dal_final), str(ial_final), str(merged_path))

        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "effect_computation", "time_sec": stage_time})
        logger.info(f"âœ… é‚»æ¥ä½œç”¨è®¡ç®—å®Œæˆ! ç”¨æ—¶: {stage_time}ç§’")

        # ======= ç¯èŠ‚6: ç»“æœæ …æ ¼åŒ– =======
        stage_start = time.time()
        logger.info("ğŸ–¨ï¸ ç»“æœæ …æ ¼åŒ–...")
        raster_path = final_dir / f"{TARGET_YEAR}_lai_raw.tif"
        raster_crop_path = final_dir / f"{TARGET_YEAR}_lai_cropped.tif"

        merged_gdf = gpd.read_file(str(merged_path))
        rasterize_lai(merged_gdf, str(raster_path), resolution=RESOLUTION)
        crop_raster(str(raster_path), str(std_path), str(raster_crop_path))

        stage_time = round(time.time() - stage_start, 2)
        stage_logs.append({"stage": "rasterization", "time_sec": stage_time})
        logger.info(f"âœ… ç»“æœæ …æ ¼åŒ–å®Œæˆ! ç”¨æ—¶: {stage_time}ç§’")

        # æ€»è®¡æ—¶
        elapsed = round(time.time() - start_time, 2)
        all_times.append({
            "year": "all",
            "status": "Success",
            "runtime_sec": elapsed,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        logger.info(f"ğŸ å…¨æµç¨‹å®Œæˆ! æ€»ç”¨æ—¶: {elapsed}ç§’")

    except TimeoutError as e:
        # æ•è·è¶…æ—¶å¼‚å¸¸
        error_msg = f"å…¨å±€è®¡ç®—è¶…æ—¶: {str(e)}"
        logger.error(error_msg)
        all_times.append({
            "year": "all",
            "status": "Failed",
            "error": str(e),
            "runtime_sec": round(time.time() - start_time, 2),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        log_path = OUTPUT_ROOT / "logs" / "failure.log"
        with open(str(log_path), "w", encoding='utf-8') as f:
            f.write(error_msg)
        logger.error(f"[ERROR] æµç¨‹å¤±è´¥: {error_msg}")
    except Exception as e:
        # é”™è¯¯å¤„ç†
        error_msg = traceback.format_exc()
        all_times.append({
            "year": "all",
            "status": "Failed",
            "error": str(e),
            "runtime_sec": round(time.time() - start_time, 2),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        log_path = OUTPUT_ROOT / "logs" / "failure.log"
        with open(str(log_path), "w", encoding='utf-8') as f:
            f.write(error_msg)
        clean_error = str(e).replace('âŒ', '[ERROR]')
        logger.error(f"[ERROR] æµç¨‹å¤±è´¥: {clean_error}")
    finally:
        # å–æ¶ˆè¶…æ—¶è®¡æ—¶å™¨
        timer.cancel()

    # ä¿å­˜æ—¥å¿—ï¼ˆPathå¯¹è±¡å…¼å®¹ï¼‰
    runtime_log = OUTPUT_ROOT / "logs" / "runtime_summary.csv"
    stage_log = OUTPUT_ROOT / "logs" / "stage_times.csv"
    pd.DataFrame(all_times).to_csv(str(runtime_log), index=False)
    pd.DataFrame(stage_logs).to_csv(str(stage_log), index=False)
    logger.info(f"ğŸ“Š å·²ä¿å­˜è¿è¡Œæ—¶é—´æ—¥å¿—: {runtime_log} å’Œ {stage_log}")
    log_memory_usage()

if __name__ == "__main__":
    mp.set_start_method('spawn', force=True)
    main()